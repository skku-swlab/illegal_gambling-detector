import pandas as pd
import requests
from time import sleep
import idna

# ---------- ì„¤ì • ----------
file_path = "ì›ë³¸ë°ì´í„°/20250402_SKKU_gambling.csv"
output_path = "ì›ë³¸ë°ì´í„°/20250402_SKKU_gambling_valid_only.csv"
error_output_path = "ì›ë³¸ë°ì´í„°/20250402_SKKU_gambling_exceptions.csv"
batch_size = 100

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \
(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
}

exclude_keywords = [
    'kakao', 'docs.google.com/forms', 't.me', 'telegram',
    'naver.', 'youtube.', 'youtu.be', 'blog.naver.', 'forms.gle'
]

html_404_keywords = ['404', 'page not found', 'notfound', 'not found', 'ì¡´ì¬í•˜ì§€ ì•ŠìŒ', 'ì—†ëŠ” í˜ì´ì§€']

# ---------- ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì¤‘ë³µ ì œê±° ----------
df = pd.read_csv(file_path)
df = df[df['link_included'] == 1].copy()
df = df.drop_duplicates(subset='restored_link')
df = df[df['restored_link'].notnull()]
urls = df['restored_link'].tolist()
total = len(urls)

valid_links = []
error_links = []

print(f"ğŸ” ì´ {total}ê°œì˜ ê³ ìœ  ë§í¬ë¥¼ {batch_size}ê°œì”© ë°°ì¹˜ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n")

# ---------- ë°°ì¹˜ ì²˜ë¦¬ ----------
for i in range(0, total, batch_size):
    batch = urls[i:i + batch_size]
    print(f"ğŸ“¦ ë°°ì¹˜ {i // batch_size + 1} / {((total - 1) // batch_size) + 1} ì²˜ë¦¬ ì¤‘...")

    for j, url in enumerate(batch):
        index = i + j

        try:
            response = requests.get(url, headers=headers, timeout=5, allow_redirects=True)
            final_url = response.url

            # í•œê¸€ ë„ë©”ì¸ punycode ì²˜ë¦¬
            try:
                domain = final_url.split("/")[2]
                ascii_domain = idna.encode(domain).decode()
                final_url = final_url.replace(domain, ascii_domain)
            except:
                pass

            if not response.ok:
                print(f"  âŒ [{index}] {url} â†’ HTTP ì˜¤ë¥˜ ({response.status_code})")
                continue

            if any(excl in final_url.lower() for excl in exclude_keywords):
                print(f"  âŒ [{index}] {url} â†’ ì°¨ë‹¨ ë„ë©”ì¸ í¬í•¨ ({final_url})")
                continue

            content = response.text.lower()
            if any(keyword in content for keyword in html_404_keywords):
                print(f"  âŒ [{index}] {url} â†’ ë³¸ë¬¸ ë‚´ '404' ë˜ëŠ” 'not found' í¬í•¨ë¨")
                continue

            # âœ… ìœ íš¨í•œ ë§í¬
            valid_links.append({'restored_link': url})
            print(f"  âœ… [{index}] {url} â†’ OK â†’ {final_url}")

        except Exception as e:
            print(f"  âš ï¸ [{index}] {url} â†’ ì ‘ì† ì‹¤íŒ¨ (ìœ ì§€): {e}")
            valid_links.append({'restored_link': url})
            error_links.append({'restored_link': url})
            continue

    sleep(1)

# ---------- ê²°ê³¼ ì €ì¥ (ë§í¬ë§Œ ì €ì¥) ----------
pd.DataFrame(valid_links).to_csv(output_path, index=False)
pd.DataFrame(error_links).to_csv(error_output_path, index=False)

print(f"\nâœ… ì™„ë£Œ! ìœ íš¨í•œ ë§í¬ {len(valid_links)}ê°œ ì €ì¥ë¨ â†’ {output_path}")
print(f"âš ï¸ ì˜ˆì™¸ ë°œìƒ ë§í¬ {len(error_links)}ê°œ ë”°ë¡œ ì €ì¥ë¨ â†’ {error_output_path}")
