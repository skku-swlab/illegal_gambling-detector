from bs4 import BeautifulSoup
from urllib.parse import urljoin
from PIL import Image
from io import BytesIO
import requests
import uuid
import json
import time
from threading import Thread, Event
from PIL import Image, UnidentifiedImageError
from io import BytesIO
import requests
import re
import csv
from datetime import datetime
import logging
import os
import signal
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from colorama import init, Fore, Back, Style
import psycopg2
from psycopg2.extras import RealDictCursor
from dotenv import load_dotenv

# ì»¬ëŸ¬ ì¶œë ¥ ì´ˆê¸°í™”
init(autoreset=True)

# ì „ì—­ ë³€ìˆ˜
should_stop = Event()
total_found = 0
processed_links = 0
total_links = 0

# ì‹¤í–‰ì‹œë§ˆë‹¤ ìƒˆë¡œìš´ ë¡œê·¸ íŒŒì¼ ìƒì„±
def setup_logging():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"gambling_detector_{timestamp}.log"
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # ìƒˆë¡œìš´ ë¡œê·¸ ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            # ì½˜ì†” ë¡œê¹…ì€ ë³„ë„ë¡œ ì²˜ë¦¬
        ]
    )
    
    logging.info(f"ìƒˆë¡œìš´ ë¡œê·¸ íŒŒì¼ ìƒì„±: {log_filename}")
    return log_filename

# í”„ë¡œê·¸ë¨ ì‹œì‘ì‹œ ë¡œê·¸ ì„¤ì •
current_log_file = setup_logging()

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Naver Clova OCR API ì„¤ì • (í™˜ê²½ ë³€ìˆ˜)
CLOVA_OCR_URL = os.getenv('CLOVA_OCR_URL', '')
CLOVA_SECRET_KEY = os.getenv('CLOVA_SECRET_KEY', '')

# Google Custom Search API ì„¤ì • (í™˜ê²½ ë³€ìˆ˜)
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY', '')
SEARCH_ENGINE_ID = os.getenv('SEARCH_ENGINE_ID', '')

# PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜)
DB_CONFIG = {
    "host": os.getenv("DB_HOST", ""),
    "user": os.getenv("DB_USER", ""),
    "port": int(os.getenv("DB_PORT", "5432")),
    "database": os.getenv("DB_NAME", ""),
    "password": os.getenv("DB_PASSWORD", ""),
    "sslmode": os.getenv("DB_SSLMODE", "require")
}

# í†µí•© CSV íŒŒì¼ëª… ì„¤ì •
MASTER_CSV_FILE = "gambling_detection_results.csv"
CRAWLED_LINKS_CSV_FILE = "crawled_links.csv"

# ë„ë°• ê´€ë ¨ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
GAMBLING_KEYWORDS = [
    "ì¹´ì§€ë…¸", "ë² íŒ…", "ë°°ë‹¹ë¥ ", "ìŠ¬ë¡¯", "ê²Œì„ë¨¸ë‹ˆ", "ë¼ì´ë¸Œ ë”œëŸ¬",
    "ê°€ì… ë³´ë„ˆìŠ¤", "í™˜ì „", "ë¬´ë£Œ ë¨¸ë‹ˆ", "ë°°ë‹¹", "í˜ì´ë°±",
    "ì½¤í”„", "ì¬ì…í”Œ", "ì…í”Œ", "ì „ì „", "íœ´ê²Œì†Œ", "ë§¤ì¶©",
    "ì¬ì¶©ì „", "ì²«ì¶©", "ì²«ì…ê¸ˆ ë³´ë„ˆìŠ¤", "ë§¤ì¹­ ë³´ë„ˆìŠ¤", "ì…ê¸ˆ ë³´ë„ˆìŠ¤", "ì¶œê¸ˆ ë³´ë„ˆìŠ¤", "ì¿ í°", "ì¶”ì²œì¸ ì½”ë“œ",
    "ì¦‰ì‹œ í™˜ì „", "ë¹ ë¥¸ í™˜ì „", "ê³ ë°°ë‹¹", "ì •ì‚°", "ë¹ ë¥¸ ì¶œê¸ˆ",
    "ë£°ë ›", "ë¸”ë™ì­", "í¬ì»¤", "í™€ë¤", "ë°”ì¹´ë¼", "ë„ë°•", "ìŠ¬ë¡¯ ë¨¸ì‹ ",
    "VIP í˜œíƒ", "ë¬´í•œ ë³´ìƒ", "ë¡œì—´í‹° í”„ë¡œê·¸ë¨", "í† í† ", "ì…í”Œ", "ë†€ì´í„°"
]

def print_header():
    """í”„ë¡œê·¸ë¨ í—¤ë” ì¶œë ¥"""
    print("\n" + "="*80)
    print(f"{Fore.CYAN}{Style.BRIGHT}Gambling-X Console Version")
    print(f"{Fore.YELLOW}ë¶ˆë²• ë„ë°• ì‚¬ì´íŠ¸ íƒì§€ í”„ë¡œê·¸ë¨")
    print("="*80)
    print(f"{Fore.WHITE}ë¡œê·¸ íŒŒì¼: {current_log_file}")
    print(f"ê²°ê³¼ íŒŒì¼: {MASTER_CSV_FILE}")
    print(f"íƒì§€ í‚¤ì›Œë“œ: {len(GAMBLING_KEYWORDS)}ê°œ")
    print("="*80)

def print_status(message, level="INFO"):
    """ìƒíƒœ ë©”ì‹œì§€ ì¶œë ¥"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    
    if level == "SUCCESS":
        print(f"{Fore.GREEN}[{timestamp}] âœ… {message}")
    elif level == "WARNING":
        print(f"{Fore.YELLOW}[{timestamp}] âš ï¸  {message}")
    elif level == "ERROR":
        print(f"{Fore.RED}[{timestamp}] âŒ {message}")
    elif level == "FOUND":
        print(f"{Fore.MAGENTA}[{timestamp}] ğŸ¯ {message}")
    else:
        print(f"{Fore.CYAN}[{timestamp}] â„¹ï¸  {message}")

def print_progress():
    """ì§„í–‰ ìƒí™© ì¶œë ¥"""
    global processed_links, total_links, total_found
    if total_links > 0:
        progress = (processed_links / total_links) * 100
        print(f"\r{Fore.BLUE}ì§„í–‰ë¥ : {progress:.1f}% ({processed_links}/{total_links}) | íƒì§€ëœ ì‚¬ì´íŠ¸: {total_found}ê°œ", end="", flush=True)

# ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ (Ctrl+C ì²˜ë¦¬)
def signal_handler(sig, frame):
    print_status("\ní”„ë¡œê·¸ë¨ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.", "WARNING")
    should_stop.set()
    print_status("ì•ˆì „í•˜ê²Œ ì¢…ë£Œ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.", "INFO")

signal.signal(signal.SIGINT, signal_handler)

# PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•¨ìˆ˜
def get_db_connection():
    """PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ë°˜í™˜"""
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        return conn
    except psycopg2.Error as e:
        logging.error(f"[DB ERROR] ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        print_status(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {str(e)}", "ERROR")
        return None

def check_url_exists(url):
    """URLì´ ì´ë¯¸ ë°ì´í„°ë² ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
    conn = get_db_connection()
    if not conn:
        return False
    
    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT COUNT(*) FROM crawled_sites WHERE url = %s", (url,))
            count = cursor.fetchone()[0]
            return count > 0
    except psycopg2.Error as e:
        logging.error(f"[DB ERROR] URL ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        return False
    finally:
        conn.close()

def save_url_to_db(url, search_keyword):
    """ìƒˆë¡œìš´ URLì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ì¤‘ë³µ í™•ì¸ í›„ ì €ì¥)"""
    conn = get_db_connection()
    if not conn:
        return False
    
    try:
        with conn.cursor() as cursor:
            # ë¨¼ì € URLì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            cursor.execute("SELECT COUNT(*) FROM crawled_sites WHERE url = %s", (url,))
            count = cursor.fetchone()[0]
            
            if count > 0:
                logging.info(f"[DB SKIP] ì´ë¯¸ ì¡´ì¬í•˜ëŠ” URL: {url}")
                return False  # ì´ë¯¸ ì¡´ì¬í•˜ë¯€ë¡œ ì €ì¥í•˜ì§€ ì•ŠìŒ
            
            # ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œ ì €ì¥ (platform='banner', keyword=ê²€ìƒ‰í‚¤ì›Œë“œ)
            cursor.execute(
                "INSERT INTO crawled_sites (url, platform, keyword, crawled_at) VALUES (%s, %s, %s, CURRENT_TIMESTAMP)",
                (url, 'banner', search_keyword)
            )
            conn.commit()
            logging.info(f"[DB SUCCESS] ìƒˆ URL ì €ì¥ ì™„ë£Œ: {url}, í”Œë«í¼: banner, í‚¤ì›Œë“œ: {search_keyword}")
            return True
            
    except psycopg2.Error as e:
        logging.error(f"[DB ERROR] URL ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        conn.rollback()
        return False
    finally:
        conn.close()

def save_urls_batch_to_db(urls, search_keyword):
    """ì—¬ëŸ¬ URLì„ í•œ ë²ˆì— ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
    if not urls:
        return 0
    
    conn = get_db_connection()
    if not conn:
        return 0
    
    saved_count = 0
    try:
        with conn.cursor() as cursor:
            for url in urls:
                # ì¤‘ë³µ í™•ì¸ í›„ ì €ì¥
                cursor.execute("SELECT COUNT(*) FROM crawled_sites WHERE url = %s", (url,))
                if cursor.fetchone()[0] == 0:
                    cursor.execute(
                        "INSERT INTO crawled_sites (url, platform, keyword, crawled_at) VALUES (%s, %s, %s, CURRENT_TIMESTAMP)",
                        (url, 'banner', search_keyword)
                    )
                    saved_count += 1
                    logging.info(f"[DB SUCCESS] ìƒˆ URL ì €ì¥: {url}, í”Œë«í¼: banner, í‚¤ì›Œë“œ: {search_keyword}")
                else:
                    logging.info(f"[DB SKIP] ì´ë¯¸ ì¡´ì¬í•˜ëŠ” URL: {url}")
        
        conn.commit()
        print_status(f"ë°ì´í„°ë² ì´ìŠ¤ì— {saved_count}ê°œì˜ ìƒˆ URL ì €ì¥ ì™„ë£Œ", "SUCCESS")
        
    except psycopg2.Error as e:
        logging.error(f"[DB ERROR] ë°°ì¹˜ URL ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        conn.rollback()
        print_status(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {str(e)}", "ERROR")
    finally:
        conn.close()
    
    return saved_count

# ë„ë°• ì‚¬ì´íŠ¸ ì—¬ë¶€ íŒë‹¨ í•¨ìˆ˜
def is_gambling_content(text):
    for keyword in GAMBLING_KEYWORDS:
        if keyword in text:
            logging.info(f"[ë„ë°• í‚¤ì›Œë“œ ê°ì§€] '{keyword}'")
            return True, keyword
    return False, None

Image.MAX_IMAGE_PIXELS = None

# ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜
def extract_text_clova(image_url):
    logging.info(f"[OCR ì²˜ë¦¬] {image_url}")

    # ë°ì´í„° URL ë˜ëŠ” SVG ì´ë¯¸ì§€ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.
    if re.match(r"^data:image/", image_url) or image_url.endswith(".svg"):
        logging.info(f"[ê±´ë„ˆëœ€] ë°ì´í„° URL ë˜ëŠ” SVG ì´ë¯¸ì§€")
        return None

    headers = {
        'X-OCR-SECRET': CLOVA_SECRET_KEY,
    }

    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    try:
        image_response = requests.get(image_url, timeout=10)
        if image_response.status_code != 200:
            logging.error(f"[ì˜¤ë¥˜] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {image_url}")
            return None
        logging.info(f"[ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ] ì„±ê³µ")
    except requests.exceptions.RequestException as e:
        logging.error(f"[ERROR] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

    # GIF ì´ë¯¸ì§€ì¼ ê²½ìš° ì²« ë²ˆì§¸ í”„ë ˆì„ ì¶”ì¶œ
    try:
        image = Image.open(BytesIO(image_response.content))
    except UnidentifiedImageError:
        logging.error(f"[ERROR] ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ íŒŒì¼: {image_url}")
        return None
    except Exception as e:
        logging.error(f"[ERROR] ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

    if image.format == "GIF":
        logging.info(f"[INFO] GIF ì´ë¯¸ì§€ ì²« ë²ˆì§¸ í”„ë ˆì„ ì¶”ì¶œ ì¤‘: {image_url}")
        image = image.convert("RGB")
        with BytesIO() as output:
            image.save(output, format="JPEG")
            image_content = output.getvalue()
    else:
        image_content = image_response.content

    # Clova OCR ìš”ì²­
    request_json = {
        'images': [{'format': 'jpg', 'name': 'demo'}],
        'requestId': str(uuid.uuid4()),
        'version': 'V2',
        'timestamp': int(round(time.time() * 1000))
    }

    # ìš”ì²­ ë°ì´í„° ë° íŒŒì¼ ì„¤ì •
    payload = {'message': json.dumps(request_json).encode('UTF-8')}
    files = [
        ('file', ('image.jpg', image_content, 'image/jpeg'))
    ]

    # Clova OCR API í˜¸ì¶œ
    try:
        response = requests.post(CLOVA_OCR_URL, headers=headers, data=payload, files=files)
        if response.status_code != 200:
            logging.error(f"[ERROR] OCR ìš”ì²­ ì‹¤íŒ¨: {response.status_code}, ì‘ë‹µ ë‚´ìš©: {response.content}")
            return None
    except requests.exceptions.RequestException as e:
        logging.error(f"[ERROR] Clova OCR API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

    # ì‘ë‹µ JSON íŒŒì‹±
    response_json = response.json()
    logging.info(f"[INFO] OCR ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ: {response_json}")

    extracted_text = []
    for image_data in response_json.get('images', []):
        for field in image_data.get('fields', []):
            extracted_text.append(field.get('inferText', '').strip())

    if extracted_text:
        logging.info(f"[í…ìŠ¤íŠ¸ ì¶”ì¶œ] ì„±ê³µ ({len(' '.join(extracted_text))}ì)")
    else:
        logging.info(f"[í…ìŠ¤íŠ¸ ì¶”ì¶œ] ì‹¤íŒ¨")

    return " ".join(extracted_text) if extracted_text else "No text found"

# Google Custom Search APIë¥¼ í†µí•´ ë‹¤ì¤‘ ê²€ìƒ‰ì–´ ì²˜ë¦¬ í•¨ìˆ˜
def get_google_search_links(keyword, start_result=1, end_result=100):
    """
    Google Custom Search APIë¥¼ í†µí•´ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
    
    Args:
        keyword (str): ê²€ìƒ‰ í‚¤ì›Œë“œ
        start_result (int): ì‹œì‘ ê²°ê³¼ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
        end_result (int): ë ê²°ê³¼ ë²ˆí˜¸ (í¬í•¨)
    
    Returns:
        list: í•„í„°ë§ëœ ê²€ìƒ‰ ê²°ê³¼ ë§í¬ ëª©ë¡
    """
    search_url = "https://www.googleapis.com/customsearch/v1"
    results = []
    
    # ì œì™¸í•  ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸ (ë‰´ìŠ¤ ì‚¬ì´íŠ¸ + ì˜ìƒ í”Œë«í¼ + ê¸°íƒ€)
    excluded_domains = [
        # í•œêµ­ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
        "news.naver.com", "news.daum.net", "news.google.com",
        "yna.co.kr", "yonhapnews.co.kr", "newsis.com",
        "chosun.com", "joongang.co.kr", "donga.com", 
        "hani.co.kr", "khan.co.kr", "kyunghyang.com",
        "sbs.co.kr", "kbs.co.kr", "mbc.co.kr", "jtbc.joins.com",
        "ytn.co.kr", "nocutnews.co.kr", "edaily.co.kr",
        "mk.co.kr", "etnews.com", "dt.co.kr", "hankookilbo.com",
        
        # ì˜ìƒ í”Œë«í¼
        "youtube.com", "youtu.be", "tiktok.com",
        "instagram.com", "facebook.com", "twitter.com",
        "vimeo.com", "dailymotion.com", "twitch.tv",
        
        # ê¸°íƒ€ ì œì™¸ ì‚¬ì´íŠ¸
        "korea.kr", "namu.wiki", "pinterest.com", "pinterest.co.kr"
    ]
    
    logging.info(f"[Google ê²€ìƒ‰] í‚¤ì›Œë“œ: {keyword} ({start_result}-{end_result}ë²ˆ ê²°ê³¼)")

    # start_resultë¶€í„° end_resultê¹Œì§€ 10ê°œì”© ê°€ì ¸ì˜¤ê¸°
    for start in range(start_result, end_result + 1, 10):
        if should_stop.is_set():
            break
        
        # í˜„ì¬ ë°°ì¹˜ì—ì„œ ê°€ì ¸ì˜¬ ê°œìˆ˜ ê³„ì‚°
        current_end = min(start + 9, end_result)
        num_to_fetch = current_end - start + 1
        
        params = {
            "key": GOOGLE_API_KEY,
            "cx": SEARCH_ENGINE_ID,
            "q": keyword,
            "start": start,
            "num": min(10, num_to_fetch),  # ìµœëŒ€ 10ê°œ, ë‚¨ì€ ê°œìˆ˜ê°€ ì ìœ¼ë©´ ê·¸ë§Œí¼ë§Œ
            "safe": "off"      # ì„¸ì´í”„ì„œì¹˜ ë„ê¸°
        }

        try:
            response = requests.get(search_url, params=params, timeout=10)
            if response.status_code == 403:
                logging.error(f"[ERROR] Google API í• ë‹¹ëŸ‰ ì´ˆê³¼. í˜„ì¬ê¹Œì§€ {len(results)}ê°œ ìˆ˜ì§‘")
                break
            elif response.status_code != 200:
                logging.error(f"[ERROR] êµ¬ê¸€ API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                break

            data = response.json()
            items = data.get("items", [])

            if not items:
                logging.info(f"[INFO] ë” ì´ìƒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break

            # ë„ë©”ì¸ í•„í„°ë§ ì ìš©
            filtered_links = []
            for item in items:
                link = item["link"]
                # ì œì™¸í•  ë„ë©”ì¸ì´ ë§í¬ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì¶”ê°€
                if not any(domain in link for domain in excluded_domains):
                    filtered_links.append(link)
                else:
                    logging.info(f"[í•„í„°ë§ë¨] ì œì™¸ëœ ë„ë©”ì¸: {link}")
            
            results.extend(filtered_links)
            total_items = len(items)
            filtered_count = len(filtered_links)
            excluded_count = total_items - filtered_count
            
            logging.info(f"[Google ê²€ìƒ‰] {start}-{current_end} í˜ì´ì§€: ì´ {total_items}ê°œ ì¤‘ {filtered_count}ê°œ ì¶”ê°€, {excluded_count}ê°œ í•„í„°ë§ë¨ (ì´ {len(results)}ê°œ)")

            if len(items) < 10:
                break
                
            time.sleep(0.5)  # API í˜¸ì¶œ ê°„ê²© ì¶”ê°€ (rate limiting ë°©ì§€)
        except requests.exceptions.RequestException as e:
            logging.error(f"[ERROR] êµ¬ê¸€ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            break

    logging.info(f"[ê²€ìƒ‰ ê²°ê³¼] '{keyword}' í‚¤ì›Œë“œì—ì„œ {len(results)}ê°œì˜ ë§í¬ ë°œê²¬ (í•„í„°ë§ í›„)")
    
    # í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ CSVì— ì €ì¥
    if results:
        save_crawled_links_to_csv(keyword, results)
    
    return results

# ì›¹ í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ í•¨ìˆ˜
def extract_image_urls(url, html_content):
    try:
        # HTML íŒŒì‹± ì‹œ ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
        soup = BeautifulSoup(html_content, 'html.parser')
        image_data = []

        logging.info(f"[ì´ë¯¸ì§€ URL ì¶”ì¶œ] ì‹œì‘: {url}")

        # ì¼ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬
        for img_tag in soup.find_all("img"):
            try:
                parent_a = img_tag.find_parent("a")
                if parent_a and parent_a.get("href"):
                    image_url = urljoin(url, img_tag.get("src"))
                    link_url = urljoin(url, parent_a.get("href"))
                    image_data.append({
                        "image_url": image_url,
                        "link_url": link_url
                    })
            except Exception as e:
                logging.warning(f"[ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜] {url}ì—ì„œ ì´ë¯¸ì§€ íƒœê·¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                continue

        logging.info(f"[ì´ë¯¸ì§€ URL ì¶”ì¶œ] {len(image_data)}ê°œ ë°œê²¬")
        return image_data
        
    except Exception as e:
        logging.error(f"[HTML íŒŒì‹± ì˜¤ë¥˜] {url}ì—ì„œ HTML íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        return []

# í†µí•© CSV íŒŒì¼ì— ê²°ê³¼ ì €ì¥ í•¨ìˆ˜ (ìˆ˜ì •ë¨)
def save_to_master_csv(result):
    """
    ë‹¨ì¼ ë„ë°• ì‚¬ì´íŠ¸ ê²°ê³¼ë¥¼ ë§ˆìŠ¤í„° CSV íŒŒì¼ì— ì¶”ê°€
    """
    file_exists = os.path.isfile(MASTER_CSV_FILE)
    
    with open(MASTER_CSV_FILE, 'a', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = ['detection_time', 'image_url', 'link_url', 'search_keyword', 'search_link', 'extracted_text', 'detected_keyword']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        # íŒŒì¼ì´ ìƒˆë¡œ ìƒì„±ë˜ëŠ” ê²½ìš°ì—ë§Œ í—¤ë” ì‘ì„±
        if not file_exists:
            writer.writeheader()
            logging.info(f"[CSV ì´ˆê¸°í™”] ìƒˆë¡œìš´ ë§ˆìŠ¤í„° CSV íŒŒì¼ ìƒì„±: {MASTER_CSV_FILE}")
        
        # ê²°ê³¼ ë°ì´í„° ì¶”ê°€
        csv_row = {
            'detection_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'image_url': result.get('image_url', ''),
            'link_url': result.get('link_url', ''),
            'search_keyword': result.get('keyword', ''),
            'search_link': result.get('search_link', ''),
            'extracted_text': result.get('text', ''),
            'detected_keyword': result.get('detected_keyword', '')
        }
        
        writer.writerow(csv_row)
        logging.info(f"[CSV ì €ì¥] ë„ë°• ì‚¬ì´íŠ¸ ê²°ê³¼ ì¶”ê°€: {result.get('link_url', '')}")

# í‚¤ì›Œë“œë³„ ë¶„ì„ í•¨ìˆ˜
def process_keyword(keyword):
    global processed_links, total_found
    
    links = get_google_search_links(keyword)
    print_status(f"'{keyword}' í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘ - {len(links)}ê°œ ë§í¬", "INFO")
    
    for link in links:
        if should_stop.is_set():
            print_status("ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ë¶„ì„ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.", "WARNING")
            break
            
        processed_links += 1
        print_progress()
        
        logging.info(f"[ë§í¬ ë¶„ì„] {link}")
        try:
            response = requests.get(link, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
            if response.status_code == 200:
                image_data = extract_image_urls(link, response.text)
                for item in image_data:
                    if should_stop.is_set():
                        break
                    text_response = extract_text_clova(item["image_url"])
                    if text_response:
                        is_gambling, keyword_detected = is_gambling_content(text_response)
                        if is_gambling:
                            total_found += 1
                            gambling_result = {
                                "image_url": item["image_url"],
                                "link_url": item["link_url"],
                                "keyword": keyword,
                                "search_link": link,
                                "text": text_response,
                                "detected_keyword": keyword_detected
                            }
                            
                            # CSVì— ì €ì¥
                            save_to_master_csv(gambling_result)
                            
                            # ì½˜ì†”ì— ê²°ê³¼ ì¶œë ¥
                            print()  # ì§„í–‰ë¥  ì¤„ë°”ê¿ˆ
                            print_status(f"ë„ë°• ì‚¬ì´íŠ¸ ë°œê²¬! í‚¤ì›Œë“œ: '{keyword_detected}'", "FOUND")
                            print_status(f"URL: {item['link_url']}", "FOUND")
                            print_progress()  # ì§„í–‰ë¥  ë‹¤ì‹œ ì¶œë ¥
                            
                            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œë„
                            if save_url_to_db(item["link_url"], keyword):
                                print_status(f"ğŸ’¾ ìƒˆ URLì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì™„ë£Œ", "SUCCESS")
                            else:
                                print_status(f"ğŸ”„ ì´ë¯¸ ë°ì´í„°ë² ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ” URL", "WARNING")
            else:
                logging.warning(f"[ë§í¬ ì ‘ì† ì‹¤íŒ¨] {link}, ìƒíƒœ ì½”ë“œ: {response.status_code}")
            time.sleep(1)  # API í˜¸ì¶œ ì œí•œ
        except requests.exceptions.RequestException as e:
            logging.error(f"[ì˜¤ë¥˜] ë§í¬ ë¶„ì„ ì‹¤íŒ¨: {link} - {str(e)}")

    print()  # ì§„í–‰ë¥  ì¤„ë°”ê¿ˆ
    print_status(f"'{keyword}' í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ", "SUCCESS")

# í‚¤ì›Œë“œ íŒŒì¼ì—ì„œ ê²€ìƒ‰ì–´ ì½ê¸° í•¨ìˆ˜
def load_keywords_from_file(filename="search_wordlist.txt"):
    """
    search_wordlist.txt íŒŒì¼ì—ì„œ í‚¤ì›Œë“œë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.
    ê° ë¼ì¸ì´ í•˜ë‚˜ì˜ í‚¤ì›Œë“œì…ë‹ˆë‹¤.
    íŒŒì¼ ëê¹Œì§€ ëª¨ë“  í‚¤ì›Œë“œë¥¼ ë¡œë”©í•©ë‹ˆë‹¤.
    """
    keywords = []
    try:
        with open(filename, 'r', encoding='utf-8') as file:
            for line_num, line in enumerate(file, 1):
                keyword = line.strip()
                if keyword and not keyword.startswith('#'):  # ë¹ˆ ì¤„ê³¼ ì£¼ì„ ì œì™¸
                    keywords.append(keyword)
        
        logging.info(f"[í‚¤ì›Œë“œ ë¡œë”©] {filename}ì—ì„œ {len(keywords)}ê°œ í‚¤ì›Œë“œ ë¡œë”© ì™„ë£Œ (ì œí•œ ì—†ìŒ)")
        return keywords
        
    except FileNotFoundError:
        logging.error(f"[ERROR] {filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        # ê¸°ë³¸ í‚¤ì›Œë“œ ë°˜í™˜
        default_keywords = ["ë¬´ë£Œ ì›¹íˆ°", "ë¨¹íŠ€ ì—†ëŠ”", "ì‚¬ë‹¤ë¦¬ ì‚¬ì´íŠ¸ ì¶”ì²œ", "í† í†  ë†€ì´í„°", "ì…í”Œ í† í† "]
        logging.info(f"[ê¸°ë³¸ í‚¤ì›Œë“œ] ê¸°ë³¸ í‚¤ì›Œë“œ {len(default_keywords)}ê°œ ì‚¬ìš©")
        return default_keywords
        
    except Exception as e:
        logging.error(f"[ERROR] í‚¤ì›Œë“œ íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return ["ë¬´ë£Œ ì›¹íˆ°", "ë¨¹íŠ€ ì—†ëŠ”", "ì‚¬ë‹¤ë¦¬ ì‚¬ì´íŠ¸ ì¶”ì²œ"]

def test_db_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print_status("PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...", "INFO")
    conn = get_db_connection()
    if conn:
        try:
            with conn.cursor() as cursor:
                cursor.execute("SELECT version();")
                version = cursor.fetchone()[0]
                print_status(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ! ë²„ì „: {version}", "SUCCESS")
                
                # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM pg_tables 
                        WHERE tablename = 'crawled_sites'
                    );
                """)
                table_exists = cursor.fetchone()[0]
                if table_exists:
                    print_status("crawled_sites í…Œì´ë¸” í™•ì¸ë¨", "SUCCESS")
                    
                    # í˜„ì¬ ì €ì¥ëœ URL ê°œìˆ˜ í™•ì¸
                    cursor.execute("SELECT COUNT(*) FROM crawled_sites;")
                    count = cursor.fetchone()[0]
                    print_status(f"í˜„ì¬ ì €ì¥ëœ URL ê°œìˆ˜: {count}ê°œ", "INFO")
                else:
                    print_status("crawled_sites í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!", "ERROR")
                    
        except Exception as e:
            print_status(f"ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì‹¤íŒ¨: {str(e)}", "ERROR")
        finally:
            conn.close()
        return True
    else:
        print_status("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨", "ERROR")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    global total_found, processed_links
    
    # í—¤ë” ì¶œë ¥
    print_header()
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
    if not test_db_connection():
        print_status("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.", "ERROR")
        return
    
    # í‚¤ì›Œë“œ íŒŒì¼ì—ì„œ ê²€ìƒ‰ì–´ ë¡œë”©
    print_status("í‚¤ì›Œë“œ íŒŒì¼ì—ì„œ ê²€ìƒ‰ì–´ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...", "INFO")
    keywords = load_keywords_from_file()
    
    if not keywords:
        print_status("í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.", "ERROR")
        return
    
    # ëª…ë ¹ì¤„ ì¸ìˆ˜ë¡œ ì‹œì‘ ì§€ì  ë° ê²€ìƒ‰ ë²”ìœ„ ê²°ì •
    print_status(f"ì´ {len(keywords)}ê°œì˜ í‚¤ì›Œë“œê°€ ë¡œë”©ë˜ì—ˆìŠµë‹ˆë‹¤.", "INFO")
    
    start_index = 0
    search_start = 1
    search_end = 100
    
    if len(sys.argv) > 1:
        try:
            start_num = int(sys.argv[1])
            if start_num < 1 or start_num > len(keywords):
                print_status(f"ì˜ëª»ëœ í‚¤ì›Œë“œ ë²ˆí˜¸ì…ë‹ˆë‹¤. 1-{len(keywords)} ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.", "ERROR")
                print_status(f"ì‚¬ìš©ë²•: python3 app.py [í‚¤ì›Œë“œì‹œì‘ë²ˆí˜¸] [ê²€ìƒ‰ì‹œì‘ë²ˆí˜¸] [ê²€ìƒ‰ëë²ˆí˜¸]", "INFO")
                print_status(f"ì˜ˆì‹œ: python3 app.py 1 1 50  (1ë²ˆ í‚¤ì›Œë“œë¶€í„°, 1-50ë²ˆ ê²€ìƒ‰ê²°ê³¼)", "INFO")
                print_status(f"ì˜ˆì‹œ: python3 app.py 10 51 100  (10ë²ˆ í‚¤ì›Œë“œë¶€í„°, 51-100ë²ˆ ê²€ìƒ‰ê²°ê³¼)", "INFO")
                return
            start_index = start_num - 1
            print_status(f"ëª…ë ¹ì¤„ ì¸ìˆ˜ë¡œ {start_num}ë²ˆ í‚¤ì›Œë“œë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.", "INFO")
        except ValueError:
            print_status("ì˜ëª»ëœ í‚¤ì›Œë“œ ì‹œì‘ ë²ˆí˜¸ì…ë‹ˆë‹¤. ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.", "ERROR")
            print_status(f"ì‚¬ìš©ë²•: python3 app.py [í‚¤ì›Œë“œì‹œì‘ë²ˆí˜¸] [ê²€ìƒ‰ì‹œì‘ë²ˆí˜¸] [ê²€ìƒ‰ëë²ˆí˜¸]", "INFO")
            return
    else:
        print_status("ì‹œì‘ ë²ˆí˜¸ê°€ ì§€ì •ë˜ì§€ ì•Šì•„ 1ë²ˆ í‚¤ì›Œë“œë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.", "INFO")
        print_status(f"ë‹¤ìŒë¶€í„°ëŠ” 'python3 app.py [í‚¤ì›Œë“œì‹œì‘ë²ˆí˜¸] [ê²€ìƒ‰ì‹œì‘ë²ˆí˜¸] [ê²€ìƒ‰ëë²ˆí˜¸]'ë¡œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", "INFO")
    
    # ê²€ìƒ‰ ê²°ê³¼ ë²”ìœ„ ì„¤ì •
    if len(sys.argv) > 2:
        try:
            search_start = int(sys.argv[2])
            if search_start < 1:
                print_status("ê²€ìƒ‰ ì‹œì‘ ë²ˆí˜¸ëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.", "ERROR")
                return
            print_status(f"ê²€ìƒ‰ ì‹œì‘ ë²ˆí˜¸: {search_start}", "INFO")
        except ValueError:
            print_status("ì˜ëª»ëœ ê²€ìƒ‰ ì‹œì‘ ë²ˆí˜¸ì…ë‹ˆë‹¤. ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.", "ERROR")
            return
    
    if len(sys.argv) > 3:
        try:
            search_end = int(sys.argv[3])
            if search_end < search_start:
                print_status("ê²€ìƒ‰ ë ë²ˆí˜¸ëŠ” ì‹œì‘ ë²ˆí˜¸ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤.", "ERROR")
                return
            if search_end > 100:
                print_status("ê²€ìƒ‰ ë ë²ˆí˜¸ëŠ” 100 ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤. (Google API ì œí•œ)", "WARNING")
                search_end = 100
            print_status(f"ê²€ìƒ‰ ë ë²ˆí˜¸: {search_end}", "INFO")
        except ValueError:
            print_status("ì˜ëª»ëœ ê²€ìƒ‰ ë ë²ˆí˜¸ì…ë‹ˆë‹¤. ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.", "ERROR")
            return
    
    # ì‹œì‘ ì§€ì  ì •ë³´ ì¶œë ¥
    if start_index > 0:
        print_status(f"í‚¤ì›Œë“œ {start_index + 1}ë²ˆ '{keywords[start_index]}'ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.", "SUCCESS")
        print_status(f"ê±´ë„ˆë›´ í‚¤ì›Œë“œ: {start_index}ê°œ", "INFO")
    else:
        print_status("ì²« ë²ˆì§¸ í‚¤ì›Œë“œë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.", "SUCCESS")
    
    print_status("Gambling-X í”„ë¡œê·¸ë¨ì„ ì‹œì‘í•©ë‹ˆë‹¤!", "SUCCESS")
    print_status(f"ì²˜ë¦¬í•  í‚¤ì›Œë“œ: {len(keywords) - start_index}ê°œ (í‚¤ì›Œë“œë‹¹ {search_start}-{search_end}ë²ˆ ê²€ìƒ‰ê²°ê³¼)", "INFO")
    print_status("ì¤‘ë‹¨í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.", "INFO")
    
    # ì‹œì‘í•  í‚¤ì›Œë“œë“¤ ë¯¸ë¦¬ ë³´ê¸°
    remaining_keywords = keywords[start_index:]
    preview_keywords = remaining_keywords[:5]
    print_status(f"í‚¤ì›Œë“œ ë¯¸ë¦¬ë³´ê¸°: {', '.join(preview_keywords)}...", "INFO")
    print()
    
    start_time = time.time()
    total_found = 0
    processed_links = 0
    total_analyzed_links = 0
    
    # ì„ íƒëœ ì§€ì ë¶€í„° í‚¤ì›Œë“œë³„ë¡œ ê²€ìƒ‰ â†’ ì¦‰ì‹œ ë¶„ì„ ìˆ˜í–‰
    for i, keyword in enumerate(keywords[start_index:], start_index + 1):
        try:
            if should_stop.is_set():
                print_status("ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ í”„ë¡œê·¸ë¨ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.", "WARNING")
                break
                
            print_status(f"[{i}/{len(keywords)}] '{keyword}' ì²˜ë¦¬ ì‹œì‘", "INFO")
            
            # 1ë‹¨ê³„: í‚¤ì›Œë“œë¡œ ë§í¬ ê²€ìƒ‰ (ìƒˆë¡œìš´ ë§¤ê°œë³€ìˆ˜ ì‚¬ìš©)
            print_status(f"ğŸ” Google ê²€ìƒ‰ ì¤‘... ({search_start}-{search_end}ë²ˆ ê²°ê³¼)", "INFO")
            links = get_google_search_links(keyword, search_start, search_end)
            
            if not links:
                print_status(f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ", "WARNING")
                continue
                
            print_status(f"{len(links)}ê°œ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ", "SUCCESS")
            
            # 2ë‹¨ê³„: ìˆ˜ì§‘ëœ ë§í¬ë“¤ì„ ì¦‰ì‹œ ë¶„ì„
            print_status(f"ë§í¬ ë¶„ì„ ì‹œì‘...", "INFO")
            keyword_found = 0
            
            for j, link in enumerate(links, 1):
                if should_stop.is_set():
                    break
                    
                total_analyzed_links += 1
                print(f"\rë¶„ì„ ì¤‘: [{j}/{len(links)}] | ì „ì²´ íƒì§€: {total_found}ê°œ", end="", flush=True)
                
                try:
                    response = requests.get(link, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
                    if response.status_code == 200:
                        try:
                            # HTML íŒŒì‹± ë° ì´ë¯¸ì§€ ì¶”ì¶œì— ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€
                            image_data = extract_image_urls(link, response.text)
                            for item in image_data:
                                if should_stop.is_set():
                                    break
                                try:
                                    text_response = extract_text_clova(item["image_url"])
                                    if text_response:
                                        is_gambling, keyword_detected = is_gambling_content(text_response)
                                        if is_gambling:
                                            total_found += 1
                                            keyword_found += 1
                                            gambling_result = {
                                                "image_url": item["image_url"],
                                                "link_url": item["link_url"],
                                                "keyword": keyword,
                                                "search_link": link,
                                                "text": text_response,
                                                "detected_keyword": keyword_detected
                                            }
                                            
                                            # CSVì— ì¦‰ì‹œ ì €ì¥
                                            save_to_master_csv(gambling_result)
                                            
                                            # ë„ë°• ì‚¬ì´íŠ¸ë¡œ íƒì§€ëœ URLì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                                            print()  # ì§„í–‰ë¥  ì¤„ë°”ê¿ˆ
                                            print_status(f"ë„ë°• ì‚¬ì´íŠ¸ ë°œê²¬! í‚¤ì›Œë“œ: '{keyword_detected}'", "FOUND")
                                            print_status(f"URL: {item['link_url']}", "FOUND")
                                            
                                            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œë„
                                            if save_url_to_db(item["link_url"], keyword):
                                                print_status(f"ìƒˆ URLì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì™„ë£Œ", "SUCCESS")
                                            else:
                                                print_status(f"ì´ë¯¸ ë°ì´í„°ë² ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ” URL", "WARNING")
                                except Exception as ocr_e:
                                    logging.error(f"[OCR ì˜¤ë¥˜] {item.get('image_url', 'unknown')}ì—ì„œ OCR ì²˜ë¦¬ ì‹¤íŒ¨: {str(ocr_e)}")
                                    continue
                        except Exception as html_e:
                            logging.error(f"[HTML íŒŒì‹± ì˜¤ë¥˜] {link}ì—ì„œ HTML íŒŒì‹± ì‹¤íŒ¨: {str(html_e)}")
                            continue
                    else:
                        logging.warning(f"[ë§í¬ ì ‘ì† ì‹¤íŒ¨] {link}, ìƒíƒœ ì½”ë“œ: {response.status_code}")
                    time.sleep(1)  # API í˜¸ì¶œ ì œí•œ
                except requests.exceptions.RequestException as e:
                    logging.error(f"[ìš”ì²­ ì˜¤ë¥˜] ë§í¬ ë¶„ì„ ì‹¤íŒ¨: {link} - {str(e)}")
                    continue
                except Exception as e:
                    logging.error(f"[ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜] ë§í¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {link} - {str(e)}")
                    continue
            
            print()  # ì§„í–‰ë¥  ì¤„ë°”ê¿ˆ
            
            # í‚¤ì›Œë“œë³„ ê²°ê³¼ ìš”ì•½
            if keyword_found > 0:
                print_status(f"'{keyword}' ì™„ë£Œ: {keyword_found}ê°œ ë„ë°•ì‚¬ì´íŠ¸ íƒì§€", "SUCCESS")
            else:
                print_status(f"'{keyword}' ì™„ë£Œ: ë„ë°•ì‚¬ì´íŠ¸ íƒì§€ ì—†ìŒ", "SUCCESS")
                
            print_status(f"í˜„ì¬ê¹Œì§€ ì´ íƒì§€: {total_found}ê°œ ({i}/{len(keywords)} í‚¤ì›Œë“œ ì™„ë£Œ)", "INFO")
            
            # API í• ë‹¹ëŸ‰ ê´€ë¦¬
            if i % 2 == 0:
                print_status("â±API í˜¸ì¶œ ì œí•œì„ ìœ„í•´ ì ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤...", "INFO")
                time.sleep(3)
            
            print("-" * 60)  # êµ¬ë¶„ì„ 
        except Exception as keyword_e:
            logging.error(f"[í‚¤ì›Œë“œ ì²˜ë¦¬ ì˜¤ë¥˜] '{keyword}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(keyword_e)}")
            print_status(f"'{keyword}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ë‹¤ìŒ í‚¤ì›Œë“œë¡œ ê³„ì†...", "ERROR")
            continue
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    print("\n" + "="*80)
    print(f"{Fore.GREEN}{Style.BRIGHT}ë¶„ì„ ì™„ë£Œ!")
    print(f"{Fore.YELLOW}ìµœì¢… ê²°ê³¼:")
    print(f"   ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
    print(f"   ì²˜ë¦¬ëœ í‚¤ì›Œë“œ: {i}/{len(keywords)}ê°œ")
    print(f"   ë¶„ì„ëœ ë§í¬: {total_analyzed_links}ê°œ")
    print(f"   íƒì§€ëœ ë„ë°• ì‚¬ì´íŠ¸: {total_found}ê°œ")
    print(f"   ê²°ê³¼ íŒŒì¼: {MASTER_CSV_FILE}")
    print(f"   ë¡œê·¸ íŒŒì¼: {current_log_file}")
    print("="*80)
    
    if total_found > 0:
        detection_rate = (total_found / total_analyzed_links * 100) if total_analyzed_links > 0 else 0
        print_status(f"{total_found}ê°œì˜ ë¶ˆë²• ë„ë°• ì‚¬ì´íŠ¸ê°€ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤! (íƒì§€ìœ¨: {detection_rate:.1f}%)", "WARNING")
        print_status(f"ìƒì„¸ ê²°ê³¼ëŠ” '{MASTER_CSV_FILE}' íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.", "INFO")
    else:
        print_status("íƒì§€ëœ ë„ë°• ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.", "SUCCESS")

# í¬ë¡¤ë§ëœ ë§í¬ë¥¼ CSVì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_crawled_links_to_csv(keyword, links):
    """
    í‚¤ì›Œë“œë³„ë¡œ í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ CSV íŒŒì¼ì— ì €ì¥
    """
    file_exists = os.path.isfile(CRAWLED_LINKS_CSV_FILE)
    
    with open(CRAWLED_LINKS_CSV_FILE, 'a', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = ['crawl_time', 'keyword', 'link_url', 'link_index']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        # íŒŒì¼ì´ ìƒˆë¡œ ìƒì„±ë˜ëŠ” ê²½ìš°ì—ë§Œ í—¤ë” ì‘ì„±
        if not file_exists:
            writer.writeheader()
            logging.info(f"[CSV ì´ˆê¸°í™”] ìƒˆë¡œìš´ í¬ë¡¤ë§ ë§í¬ CSV íŒŒì¼ ìƒì„±: {CRAWLED_LINKS_CSV_FILE}")
            print_status(f"í¬ë¡¤ë§ ë§í¬ ì €ì¥ íŒŒì¼ ìƒì„±: {CRAWLED_LINKS_CSV_FILE}", "INFO")
        
        # í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ê°ê° ì €ì¥
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        for i, link in enumerate(links, 1):
            csv_row = {
                'crawl_time': current_time,
                'keyword': keyword,
                'link_url': link,
                'link_index': i
            }
            writer.writerow(csv_row)
        
        logging.info(f"[CSV ì €ì¥] '{keyword}' í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§ëœ {len(links)}ê°œ ë§í¬ ì €ì¥ ì™„ë£Œ")
        print_status(f"'{keyword}' í‚¤ì›Œë“œ ë§í¬ {len(links)}ê°œ ì €ì¥ ì™„ë£Œ", "SUCCESS")

if __name__ == '__main__':
    try:
        # colorama ëª¨ë“ˆì´ ì—†ìœ¼ë©´ ì„¤ì¹˜ ì•ˆë‚´
        try:
            from colorama import init, Fore, Back, Style
            init(autoreset=True)
        except ImportError:
            print("ì»¬ëŸ¬ ì¶œë ¥ì„ ìœ„í•´ colorama ëª¨ë“ˆì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
            print("pip install colorama")
            # colorama ì—†ì´ë„ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
            class DummyColor:
                def __getattr__(self, name):
                    return ""
            Fore = Back = Style = DummyColor()
        
        logging.info(f"Gambling-X Console í”„ë¡œê·¸ë¨ ì‹œì‘ - ë¡œê·¸ íŒŒì¼: {current_log_file}")
        main()
    except KeyboardInterrupt:
        print_status("\ní”„ë¡œê·¸ë¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.", "WARNING")
    except Exception as e:
        print_status(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", "ERROR")
        logging.error(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    finally:
        print_status("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.", "INFO")