import asyncio
from playwright.async_api import async_playwright
import random
from datetime import datetime
import urllib.parse
import re
from bs4 import BeautifulSoup
import csv
import os

# ë„ë°• í‚¤ì›Œë“œ ë¶ˆëŸ¬ì˜¤ê¸°
with open("wordlist.txt", encoding="utf-8") as f:
    keywords = [line.strip() for line in f if line.strip()]
random.shuffle(keywords)

# CSV ì´ˆê¸°í™”
CSV_FILE = "fb_detected_posts.csv"
if not os.path.exists(CSV_FILE):
    with open(CSV_FILE, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f, quoting=csv.QUOTE_ALL)
        writer.writerow(["timestamp", "keyword", "content", "links", "handles", "codes"])

seen_posts = set()

async def run():
    async with async_playwright() as p:
        browser = await p.chromium.connect_over_cdp("http://localhost:9222")
        context = browser.contexts[0]
        page = await context.new_page()  # â† ìƒˆ íƒ­ ìƒì„± (ì¶©ëŒ ë°©ì§€)
        await page.bring_to_front()

        while True:
            keyword = random.choice(keywords)
            print(f"[ğŸ”] ê²€ìƒ‰ ì¤‘: {keyword}")
            url = f"https://www.facebook.com/search/posts?q={urllib.parse.quote(keyword)}"
            await page.goto(url)
            await page.wait_for_timeout(5000)

            for i in range(8):
                await page.evaluate("window.scrollBy(0, document.body.scrollHeight)")
                print(f"  â†³ [{i+1}] ìŠ¤í¬ë¡¤ ë‚´ë¦¼")
                await page.wait_for_timeout(4000)

            await page.evaluate("""() => {
                document.querySelectorAll('div[role="button"]').forEach(el => {
                    if (el.innerText.includes('ë” ë³´ê¸°')) {
                        el.click();
                    }
                });
            }""")
            await page.wait_for_timeout(2000)

            posts = await page.query_selector_all("div[data-ad-preview='message']")
            print(f"  â†³ ê°ì§€ëœ ê²Œì‹œë¬¼ ìˆ˜: {len(posts)}")

            for post in posts:
                try:
                    html = await post.inner_html()
                    text = BeautifulSoup(html, "html.parser").get_text(separator=" ", strip=True)

                    urls_http = re.findall(r'https?://[^\s]+', text)
                    urls_obf = re.findall(r'[\wê°€-í£\.\-]+\s*\.\s*(?:com|net|org|co|kr)', text, re.IGNORECASE)
                    links = urls_http + urls_obf

                    handles = re.findall(r'@\w{3,}', text)
                    codes = re.findall(r'ê°€ì…\s*ì½”ë“œ[:ï¼š]?\s*[\w\-]+', text, re.IGNORECASE)

                    if not any(keyword in text for keyword in keywords):
                        continue
                    if not (links or handles or codes):
                        continue
                    if text in seen_posts:
                        continue
                    seen_posts.add(text)

                    print(f"[ğŸ’¬] {text}")
                    print(f"ğŸ”— ë§í¬: {', '.join(links)}")
                    print(f"ğŸ“± ì•„ì´ë””: {', '.join(handles)}")
                    print(f"ğŸ” ê°€ì…ì½”ë“œ: {', '.join(codes)}\n{'-'*60}")

                    # CSV ì €ì¥
                    with open(CSV_FILE, "a", newline="", encoding="utf-8-sig") as f:
                        writer = csv.writer(f, quoting=csv.QUOTE_ALL)
                        writer.writerow([
                            datetime.now().isoformat(),
                            keyword,
                            text,
                            "; ".join(links),
                            "; ".join(handles),
                            "; ".join(codes)
                        ])

                except Exception as e:
                    print(f"[âš ï¸] ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

            await asyncio.sleep(15)

asyncio.run(run())
